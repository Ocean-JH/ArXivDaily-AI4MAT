[
  {
    "id": "http://arxiv.org/abs/2509.25807v1",
    "title": "Fine-Tuning Bulk-oriented Universal Interatomic Potentials for Surfaces: Accuracy, Efficiency, and Forgetting Control",
    "authors": [
      "Jaekyun Hwang",
      "Taehun Lee",
      "Yonghyuk Lee",
      "Su-Hyun Yoo"
    ],
    "summary": "Accurate prediction of surface energies and stabilities is essential for\nmaterials design, yet first-principles calculations remain computationally\nexpensive and most existing interatomic potentials are trained only on bulk\nsystems. Here, we demonstrate that fine-tuning foundation machine learning\npotentials (MLPs) significantly improves both computational efficiency and\npredictive accuracy for surface modeling. While existing universal interatomic\npotentials (UIPs) have been solely trained and validated on bulk datasets, we\nextend their applicability to complex and scientifically significant unary,\nbinary, and ternary surface systems. We systematically compare models trained\nfrom scratch, zero-shot inference, conventional fine-tuning, and multi-head\nfine-tuning approach that enhances transferability and mitigates catastrophic\nforgetting. Fine-tuning consistently reduces prediction errors with\norders-of-magnitude fewer training configurations, and multi-head fine-tuning\ndelivers robust and generalizable predictions even for materials beyond the\ninitial training domain. These findings offer practical guidance for leveraging\npre-trained MLPs to accelerate surface modeling and highlight a scalable path\ntoward data-efficient, next-generation atomic-scale simulations in\ncomputational materials science.",
    "published": "2025-09-30T05:28:06+00:00",
    "updated": "2025-09-30T05:28:06+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "Main text: 21 pages, 6 figures, Supplementary information: 10 pages,\n  5 figures",
    "journal_ref": null,
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2509.25807v1",
    "base_id": "2509.25807",
    "version": "1",
    "url": "http://arxiv.org/abs/2509.25807v1"
  },
  {
    "id": "http://arxiv.org/abs/2509.25538v1",
    "title": "Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization",
    "authors": [
      "Marcus Schwarting",
      "Logan Ward",
      "Nathaniel Hudson",
      "Xiaoli Yan",
      "Ben Blaiszik",
      "Santanu Chaudhuri",
      "Eliu Huerta",
      "Ian Foster"
    ],
    "summary": "Generative AI poses both opportunities and risks for solving inverse design\nproblems in the sciences. Generative tools provide the ability to expand and\nrefine a search space autonomously, but do so at the cost of exploring\nlow-quality regions until sufficiently fine tuned. Here, we propose a queue\nprioritization algorithm that combines generative modeling and active learning\nin the context of a distributed workflow for exploring complex design spaces.\nWe find that incorporating an active learning model to prioritize top design\ncandidates can prevent a generative AI workflow from expending resources on\nnonsensical candidates and halt potential generative model decay. For an\nexisting generative AI workflow for discovering novel molecular structure\ncandidates for carbon capture, our active learning approach significantly\nincreases the number of high-quality candidates identified by the generative\nmodel. We find that, out of 1000 novel candidates, our workflow without active\nlearning can generate an average of 281 high-performing candidates, while our\nproposed prioritization with active learning can generate an average 604\nhigh-performing candidates.",
    "published": "2025-09-29T21:51:13+00:00",
    "updated": "2025-09-29T21:51:13+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": null,
    "journal_ref": null,
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2509.25538v1",
    "base_id": "2509.25538",
    "version": "1",
    "url": "http://arxiv.org/abs/2509.25538v1"
  },
  {
    "id": "http://arxiv.org/abs/2509.25186v1",
    "title": "Guided Diffusion for the Discovery of New Superconductors",
    "authors": [
      "Pawan Prakash",
      "Jason B. Gibson",
      "Zhongwei Li",
      "Gabriele Di Gianluca",
      "Juan Esquivel",
      "Eric Fuemmeler",
      "Benjamin Geisler",
      "Jung Soo Kim",
      "Adrian Roitberg",
      "Ellad B. Tadmor",
      "Mingjie Liu",
      "Stefano Martiniani",
      "Gregory R. Stewart",
      "James J. Hamlin",
      "Peter J. Hirschfeld",
      "Richard G. Hennig"
    ],
    "summary": "The inverse design of materials with specific desired properties, such as\nhigh-temperature superconductivity, represents a formidable challenge in\nmaterials science due to the vastness of chemical and structural space. We\npresent a guided diffusion framework to accelerate the discovery of novel\nsuperconductors. A DiffCSP foundation model is pretrained on the Alexandria\nDatabase and fine-tuned on 7,183 superconductors with first principles derived\nlabels. Employing classifier-free guidance, we sample 200,000 structures, which\nlead to 34,027 unique candidates. A multistage screening process that combines\nmachine learning and density functional theory (DFT) calculations to assess\nstability and electronic properties, identifies 773 candidates with\nDFT-calculated $T_\\mathrm{c}>5$ K. Notably, our generative model demonstrates\neffective property-driven design. Our computational findings were validated\nagainst experimental synthesis and characterization performed as part of this\nwork, which highlighted challenges in sparsely charted chemistries. This\nend-to-end workflow accelerates superconductor discovery while underscoring the\nchallenge of predicting and synthesizing experimentally realizable materials.",
    "published": "2025-09-29T17:59:52+00:00",
    "updated": "2025-09-29T17:59:52+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.supr-con",
    "comment": "13 pages, 5 figures, 1 table",
    "journal_ref": null,
    "doi": null,
    "pdf_url": "http://arxiv.org/pdf/2509.25186v1",
    "base_id": "2509.25186",
    "version": "1",
    "url": "http://arxiv.org/abs/2509.25186v1"
  }
]