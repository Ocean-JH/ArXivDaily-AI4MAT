[
  {
    "id": "http://arxiv.org/abs/2506.05680v2",
    "title": "Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization",
    "authors": [
      "Tailin Zhou",
      "Zhilin Chen",
      "Wenlong Lyu",
      "Zhitang Chen",
      "Danny H. K. Tsang",
      "Jun Zhang"
    ],
    "summary": "Optimizing complex systems, from discovering therapeutic drugs to designing high-performance materials, remains a fundamental challenge across science and engineering, as the underlying rules are often unknown and costly to evaluate. Offline optimization aims to optimize designs for target scores using pre-collected datasets without system interaction. However, conventional approaches may fail beyond training data, predicting inaccurate scores and generating inferior designs. This paper introduces ManGO, a diffusion-based framework that learns the design-score manifold, capturing the design-score interdependencies holistically. Unlike existing methods that treat design and score spaces in isolation, ManGO unifies forward prediction and backward generation, attaining generalization beyond training data. Key to this is its derivative-free guidance for conditional generation, coupled with adaptive inference-time scaling that dynamically optimizes denoising paths. Extensive evaluations demonstrate that ManGO outperforms 24 single- and 10 multi-objective optimization methods across diverse domains, including synthetic tasks, robot control, material design, DNA sequence, and real-world engineering optimization.",
    "published": "2025-06-06T02:11:10+00:00",
    "updated": "2026-01-12T12:56:43+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This manuscript was accepted by npj AI",
    "journal_ref": null,
    "doi": null,
    "pdf_url": "https://arxiv.org/pdf/2506.05680v2",
    "base_id": "2506.05680",
    "version": "2",
    "url": "http://arxiv.org/abs/2506.05680v2"
  },
  {
    "id": "http://arxiv.org/abs/2601.06597v1",
    "title": "Implicit bias as a Gauge correction: Theory and Inverse Design",
    "authors": [
      "Nicola Aladrah",
      "Emanuele Ballarin",
      "Matteo Biagetti",
      "Alessio Ansuini",
      "Alberto d'Onofrio",
      "Fabio Anselmi"
    ],
    "summary": "A central problem in machine learning theory is to characterize how learning dynamics select particular solutions among the many compatible with the training objective, a phenomenon, called implicit bias, which remains only partially characterized. In the present work, we identify a general mechanism, in terms of an explicit geometric correction of the learning dynamics, for the emergence of implicit biases, arising from the interaction between continuous symmetries in the model's parametrization and stochasticity in the optimization process. Our viewpoint is constructive in two complementary directions: given model symmetries, one can derive the implicit bias they induce; conversely, one can inverse-design a wide class of different implicit biases by computing specific redundant parameterizations. More precisely, we show that, when the dynamics is expressed in the quotient space obtained by factoring out the symmetry group of the parameterization, the resulting stochastic differential equation gains a closed form geometric correction in the stationary distribution of the optimizer dynamics favoring orbits with small local volume. We compute the resulting symmetry induced bias for a range of architectures, showing how several well known results fit into a single unified framework. The approach also provides a practical methodology for deriving implicit biases in new settings, and it yields concrete, testable predictions that we confirm by numerical simulations on toy models trained on synthetic data, leaving more complex scenarios for future work. Finally, we test the implicit bias inverse-design procedure in notable cases, including biases toward sparsity in linear features or in spectral properties of the model parameters.",
    "published": "2026-01-10T15:33:09+00:00",
    "updated": "2026-01-10T15:33:09+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "v1",
    "journal_ref": null,
    "doi": null,
    "pdf_url": "https://arxiv.org/pdf/2601.06597v1",
    "base_id": "2601.06597",
    "version": "1",
    "url": "http://arxiv.org/abs/2601.06597v1"
  }
]