# ArXiv Daily - AI4MAT

**Daily automatic updates of the latest arXiv papers on AI for Materials Science (AI4MatSci).** 

Stay informed with cutting-edge research at the intersection of artificial intelligence and materials science — automatically!

## :bookmark: Related Fields

- (Computational) Materials Science
- Machine Learning
- Materials Design
- Crystal Structure Prediction
- Generative AI for Materials Discovery

## :star: Customize Yours

Let's start with a star :star:!

And then, feel free to adjust the `query` field in the file `config.json` to match your own research interests(see [arXiv API User's Manual](https://info.arxiv.org/help/api/user-manual.html#51-details-of-query-construction) for more information)!

## :handshake: Contributions

Contributions are welcome!
 Feel free to open an Issue or a Pull Request if you have ideas for improvement, new features, or better queries.

## :blue_heart: ​Acknowledge

Thank you to [arXiv](https://arxiv.org/) for use of its open access interoperability.

---

## :scroll: Paper List


<!-- ARXIV_PAPERS_START -->

## New Papers (1)

*Last updated: 2025-05-22 06:16:32 (SGT)*

### 1. Rao-Blackwell Gradient Estimators for Equivariant Denoising Diffusion

**Authors:** Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert

**Published:** 2025-02-14

**Category:** cs.LG

**ID:** 2502.09890v2

**Link:** [http://arxiv.org/abs/2502.09890v2](http://arxiv.org/abs/2502.09890v2)

**Summary:** In domains such as molecular and protein generation, physical systems exhibit
inherent symmetries that are critical to model. Two main strategies have
emerged for learning invariant distributions: designing equivariant network
architectures and using data augmentation to approximate equivariance. While
equivariant architectures preserve symmetry by design, they often involve
greater complexity and pose optimization challenges. Data augmentation, on the
other hand, offers flexibility but may fall short in fully capturing
symmetries. Our framework enhances both approaches by reducing training
variance and providing a provably lower-variance gradient estimator. We achieve
this by interpreting data augmentation as a Monte Carlo estimator of the
training gradient and applying Rao-Blackwellization. This leads to more stable
optimization, faster convergence, and reduced variance, all while requiring
only a single forward and backward pass per sample. We also present a practical
implementation of this estimator incorporating the loss and sampling procedure
through a method we call Orbit Diffusion. Theoretically, we guarantee that our
loss admits equivariant minimizers. Empirically, Orbit Diffusion achieves
state-of-the-art results on GEOM-QM9 for molecular conformation generation,
improves crystal structure prediction, and advances text-guided crystal
generation on the Perov-5 and MP-20 benchmarks. Additionally, it enhances
protein designability in protein structure generation....

---


<!-- ARXIV_PAPERS_END -->